---
layout: post
title: Blog Post 7
---

Today we're going to tackle a slightly more complex problem in computer vision. If you recall in a previous blog post we created a binary image classifier that could distinguish between a dog and a cat. This time we are gonna tackle the Modified National Institute of Standards and Technology dataset or MNIST. Its a dataset of handwritten images of the 10 digits. As you might have imagined we're going to train a model that will be able to differientiate between the handwritten digits.

This is different from the previous image classification problem from before as this time we are going to be dealing with categorical classification instead of binary.
If you'd like to attempt building your own then you can submit to https://www.kaggle.com/competitions/digit-recognizer and get ranked against other people!


```python
# We'll start with some standard imports
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import os

```


```python
# Let's go ahead and load in the data and take a look at it.

data = pd.read_csv('train.csv')
data
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>pixel0</th>
      <th>pixel1</th>
      <th>pixel2</th>
      <th>pixel3</th>
      <th>pixel4</th>
      <th>pixel5</th>
      <th>pixel6</th>
      <th>pixel7</th>
      <th>pixel8</th>
      <th>...</th>
      <th>pixel774</th>
      <th>pixel775</th>
      <th>pixel776</th>
      <th>pixel777</th>
      <th>pixel778</th>
      <th>pixel779</th>
      <th>pixel780</th>
      <th>pixel781</th>
      <th>pixel782</th>
      <th>pixel783</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>41995</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>41996</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>41997</th>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>41998</th>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>41999</th>
      <td>9</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>42000 rows Ã— 785 columns</p>
</div>



We're going to have to clean the data just a tad bit to work in our tensorflow model. As you can see this is a dataframe with 785 columns and the first column is the label of the image (i.e. it indicates which digit is represented in the image).

A quick note, the images themselves are square images. This isn't always the case but here we can assume that each image is 28x28 pixels.

So, lets seperate the labels and the images into their own dataframes.


```python
# We'll start with extracting the labels from data
labels = data['label']
labels
```




    0        1
    1        0
    2        1
    3        4
    4        0
            ..
    41995    0
    41996    1
    41997    7
    41998    6
    41999    9
    Name: label, Length: 42000, dtype: int64




```python
# Now for this to be useable later on in our tensorflow model we're going to need
# a function from tensorflow.keras.utils package. So, we'll go ahead and import it
from tensorflow.keras.utils import to_categorical
labels = to_categorical(labels, len(set(labels)))
```

Let's take a quick look at this new object.


```python
labels, labels.shape
```




    (array([[0., 1., 0., ..., 0., 0., 0.],
            [1., 0., 0., ..., 0., 0., 0.],
            [0., 1., 0., ..., 0., 0., 0.],
            ...,
            [0., 0., 0., ..., 1., 0., 0.],
            [0., 0., 0., ..., 0., 0., 0.],
            [0., 0., 0., ..., 0., 0., 1.]], dtype=float32),
     (42000, 10))




```python
type(labels)
```




    numpy.ndarray



As you can see, all we did was take our one-dimensional labels data and turned it into a numpy array. We now have a matrix in which each row has 10 columns with nine values being 0 and one value being 1. Unsurprisingly, the 1 value tells us what category each of our images belongs to, which in this case, will be the corresponding digit that the image represents.

Up next, we'll do the images themselves. We're going to have to reformat them as well so that they actually look like 28x28 matrices.


```python
images = data.drop(['label'],axis=1)
images = images.to_numpy(dtype = 'float32').reshape((42000,28,28,1))
plt.imshow(images[3])
```




    <matplotlib.image.AxesImage at 0x1909811bbc8>




    
![BP7_0]({{ site.baseurl }}/images/BP7_0.png)
    


Now we can combine the two to use in training and testing our model!


```python
data_ds = tf.data.Dataset.from_tensor_slices((images,labels))
data_ds = data_ds.batch(32)
# Take a moment to check what kind of object data_ds is now
data_ds,type(data_ds)

```




    (<BatchDataset shapes: ((None, 28, 28, 1), (None, 10)), types: (tf.float32, tf.float32)>,
     tensorflow.python.data.ops.dataset_ops.BatchDataset)




```python
# Split the data into training data and validation data.
train_size = int(0.8*len(data_ds))
val_size = int(0.2*len(data_ds))
train_ds = data_ds.take(train_size)
val_ds = data_ds.skip(train_size).take(val_size)
```

We're going to build a model but we won't really be spending much time on it. I just wanted to show how you can create a powerful model with very few layers. Again, if you would like to know more about any of the layers, please check the Tensorflow documentation. Perhaps in a future blog post we will explore some imaging concepts and algorithms.

For now, enjoy the model and sit back and relax.


```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    # input layers
    layers.InputLayer(input_shape=(28, 28, 1)),
    
    # Data augmentation layer
    layers.GaussianDropout(rate = 0.2),
    
    # Block One
    layers.LocallyConnected2D(filters = 4, kernel_size = 4,
                              padding = 'valid',activation = 'relu'),
    layers.Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'),
    layers.AveragePooling2D(),
    
    # Head
    layers.Flatten(),
    layers.Dense(10, activation='softmax'),
])
```


```python
# We're going to run and fit the model to the training data
# Remember, we're dealing with categorical data so we should pick an approriate
# loss function.

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)
history = model.fit(train_ds,
          validation_data=val_ds,
          epochs=10,verbose = True)

```

    Epoch 1/10
    1050/1050 [==============================] - 10s 10ms/step - loss: 0.3018 - accuracy: 0.9106 - val_loss: 0.1345 - val_accuracy: 0.9547
    Epoch 2/10
    1050/1050 [==============================] - 7s 7ms/step - loss: 0.1462 - accuracy: 0.9553 - val_loss: 0.1011 - val_accuracy: 0.9672
    Epoch 3/10
    1050/1050 [==============================] - 7s 7ms/step - loss: 0.1141 - accuracy: 0.9656 - val_loss: 0.0966 - val_accuracy: 0.9704
    Epoch 4/10
    1050/1050 [==============================] - 7s 7ms/step - loss: 0.0988 - accuracy: 0.9688 - val_loss: 0.0880 - val_accuracy: 0.9723
    Epoch 5/10
    1050/1050 [==============================] - 7s 7ms/step - loss: 0.0888 - accuracy: 0.9721 - val_loss: 0.0823 - val_accuracy: 0.9736
    Epoch 6/10
    1050/1050 [==============================] - 7s 7ms/step - loss: 0.0816 - accuracy: 0.9735 - val_loss: 0.0945 - val_accuracy: 0.9729
    Epoch 7/10
    1050/1050 [==============================] - 7s 7ms/step - loss: 0.0751 - accuracy: 0.9760 - val_loss: 0.0794 - val_accuracy: 0.9758
    Epoch 8/10
    1050/1050 [==============================] - 7s 7ms/step - loss: 0.0706 - accuracy: 0.9773 - val_loss: 0.0896 - val_accuracy: 0.9747
    Epoch 9/10
    1050/1050 [==============================] - 7s 7ms/step - loss: 0.0670 - accuracy: 0.9785 - val_loss: 0.0863 - val_accuracy: 0.9752
    Epoch 10/10
    1050/1050 [==============================] - 8s 8ms/step - loss: 0.0615 - accuracy: 0.9804 - val_loss: 0.0933 - val_accuracy: 0.9764
    

Let's graph the results!


```python
plt.plot(history.history["accuracy"], label = "training accuracy")
plt.plot(history.history["val_accuracy"], label = "validation accuracy")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x190c80d7d88>




    
![BP7_1]({{ site.baseurl }}/images/BP6_0.png)
    


Looks clean enough. We haven't overfitted the model or underfitted. Our validation accuracy stayed consistently around 0.97. If we kept training we may have started to diverge but we don't really need to keep training as these results are quite good for a simple model like this.
