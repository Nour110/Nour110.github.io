---
layout: post
title: Blog Post 4
---


# Introduction #

Welcome to the spectral clustering blog post. Sounds fancy right? Well let the games begin. Follow along, import the right packages, install new ones.\
\
Let's take a look at what we're trying to achieve but without spectral clustering. See if you can figure it out.


```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, 
                           shuffle=True, 
                           random_state=None, 
                           centers = 2, 
                           cluster_std = 2.0)

plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x27b3cf70d88>




    
![png](output_2_1.png)
    



```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x27b3f15a988>




    
![png](output_3_1.png)
    


Well would you look at that, it seems as though we took our data and organized them into two *clusters*.\
\
Looks like we're done here...\
Nah of course not. All we did here is organize two dimensional data.


```python
X[0]
```




    array([-7.53711826,  6.56728787])



See, each data point only has two dimensions. You probably could have done that just by looking at the graphs.

# Downfall of KMeans and simple clustering #


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x27b3ee7e488>




    
![png](output_8_1.png)
    


You can cluster this with your eyes. There's a frown and then there's a smile. Surely KMeans can read facial expressions?


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x27b3f0fd088>




    
![png](output_10_1.png)
    


Oh, well I guess not. That's okay, let's give KMeans a break, it did the best it could.\
\
We're going to have to get our hands dirty for this one.\
\
Now ask yourself, is there anything linear algebra can't do?

# Part A #
Well, here we are, about to construct a **similarity matrix**. Don't you love matrices as well?\
\
We're going to contruct the similarity matrix of $X$.\
\
We'll define a function to handle constructing the matrix. First, i'll fill you in on the properties of a similarity matrix.\
\
Let $A$ be the similarity matrix of $X$. `n = len(X)`. $A$ will be have shape `(n,n)`. Let `epsilon` be an positive real number then $A[i,j] = 1$ if $X[i]$ is within distance `epsilon` of $X[j]$ and $i != j$ otherwise $A[i,j]=0$.


```python
import sklearn
```


```python
def similarity_matrix(epsilon,X):
    # handy function to computer the pairwase distance of the points in X
    # it will return an nxn matrix
    A = sklearn.metrics.pairwise_distances(X)
    # Now we set all values above epsilon**2 to -1 
    # (we don't set them to 0 as it is possible there are two seperate data points 
    # x,y that have the same coordinates)
    A[A>epsilon**2] = -1
    # Now set all entries not equal to -1 to 1
    # (this ensures that if distance <= epsilon**2 then we set it to 1)
    A[A != -1] = 1
    # finally set all entries that were > epsilon**2 to 0
    A[A == -1] = 0
    # and we overwrite the diagonal to 0
    np.fill_diagonal(A,0)
    # convert our matrix to int
    A = A.astype(int)
    # now we'll return the matrix
    return A
```


```python
# We're gonna take epsilon = 0.4 for our example.
A = similarity_matrix(0.4,X)
A
```




    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           ...,
           [0, 0, 0, ..., 0, 0, 1],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 1, 0, 0]])



# Part B #
Alright, so now we have a similiarity matrix. Every entry in $A$ is either 0 or 1. This will tell us whether a point is distance epsilon or less from another point. Using this we can begin our process of clustering.\
\
We're going to need a couple functions to facilitate this. I'll give you the formulas of what we're looking to calculate.\
\
First, we want the $i$th row-sum of $A$ which we will call $d_i$ or the degree of $i$ with
$$d_i = \sum^n_{j=1} a_{ij}$$
Next, let $C_0$ and $C_1$ be two clusters of data points and assume that all our data points are in either $C_0$ or $C_1$. Okay, skipping over some details we're going to calculate
$$N_A(C_0,C_1) = cut(C_0,C_1)\bigg(\frac{1}{vol(C_o)}+\frac{1}{vol(C_1)}\bigg)$$
where
$$cut(C_0,C_1) = \sum_{i\in C_0,j\in C_1} a_{ij}$$
and 
$$vol(C_0) = \sum_{i\in C_0} d_i$$
\
$N_A(C_0,C_1)$ is called the **binary norm cut objective** of $A$.\
\
Don't worry too much about it if you got lost there. We're gonna write functions to compute these for us and it'll all be simple in the end.

# $cut(C_0,C_1)$ #


```python
def cut(A,y):
   # We're going to calculate the cut term
    
    # begin by setting our output to 0
    cut_output = 0
    
    # if y[i]=0 and y[j] = 1 then we will add A[i][j] to our output
    # it's as simple as that
    for i in np.where(y == 0)[0]:
        for j in np.where(y==1)[0]:
            cut_output = cut_output + A[i][j]
            
    # return the result
    return cut_output
```


```python
# Let's test our function with the actual clusters, C_0 and C_1 
# and with a randomly generated set of clusters

random_y = np.random.choice([0,1],size = len(X))
print("Cut term using actual y: "+ str(cut(A,y))+"\nCut term using randomly generated y: "
      +str(cut(A,random_y)) )
```

    Cut term using actual y: 0
    Cut term using randomly generated y: 373
    

As you can see the cut term using the actual clusters is $0$ which would be expected. As we would expect that two points from different clusters would not be connected (ie they are more than epsilon distance from each other).\
\
Let's make a function to compute the volume terms now.

# vol(A,y) #


```python
def vol(A,y):
    # This is a very simple computation.
    # We just want the sum of the rows i such that y[i] = 0
    # and the sum of the rows j such that y[j]=1
    vol_0 = A[np.where(y==0)].sum()
    vol_1 = A[np.where(y==1)].sum()
    return(vol_0,vol_1)
```

# normcut(A,y) #

Now we'll just combine all the terms to get $N_A(C_0,C_1)$ from above.


```python
def normcut(A,y):
    cutterm = cut(A,y)
    vol_0,vol_1 = vol(A,y)
    n_cut = cutterm*(1/vol_0+1/vol_1)
    return n_cut
```


```python
print("normcut with true clusters: "+str(normcut(A,y))+"\nNormcut with random clusters: "
      +str(normcut(A,random_y)))
```

    normcut with true clusters: 0.0
    Normcut with random clusters: 0.9877184588483989
    

As before, it looks like our normcut using the true clusters is equal to $0$. Which is not surprising since the cut term was $0$ as well

Nice we now have a way to compute the binary norm cut objective of $A$. Clearly, from the results, it would seem that we want $normcut$ to be as small as possible. That's some optimazation and would be computationally expensive. So, we need a different method to compute $N_A(C_0,C_1)$.

# Part C #

Let us begin the process. First, I'll provide you with the math of what we're going to do. If you get lost then remember ABC\
Always\
Be\
Rereading\
\
We begin by defining a new vector $z\in\mathbb{R}^n$ such that
$$ z_i = \begin{cases} \frac{1}{vol(C_0)} \hspace{1cm} \text{    if } y_i = 0\\ -\frac{1}{vol(C_1)}\hspace{1cm} \text{    if } y_i =1 \end{cases}$$
As you may have noticed, the sign of each element in $z$ encodes the cluster that the point belongs to.\
\
Let $D$ be the diagonal matrix with nonzero entries $d_{ii} = d_i$. Recall that $d_i$ is the $i$th row-sum of $A$. Then
$$ N_A(C_0,C_1) = \frac{z^\top(D-A)z}{z^\top Dz} $$
Neat.\
\
Let's begin by writing a function to compute our new vector $z$.
# transform(A,y) #


```python
def transform(A,y):
    # use our previous function to compute the vol terms
    v_0,v_1 = vol(A,y)
    # initialize an array z with length equal to the length of y
    z = np.zeros(len(y))
    # construct z appropriately as described above
    # if y[i]=0 then z[i] = 1/v_0 
    # if y[i]=1 then z[i] = -1/v_1
    z[np.where(y==0)] = 1/v_0
    z[np.where(y==1)] = -1/v_1
    # return z
    return z
```

Let's check if this actually works and is an alternative, more efficient, way of computing $N_A(C_0,C_1)$. We're just going to compare our new method to the results of the previous method.


```python
# We'll just first quickly construct our D matrix.
D = np.zeros(np.shape(A))
np.fill_diagonal(D,A.sum(axis = 1))
D
```




    array([[ 4.,  0.,  0., ...,  0.,  0.,  0.],
           [ 0.,  6.,  0., ...,  0.,  0.,  0.],
           [ 0.,  0.,  8., ...,  0.,  0.,  0.],
           ...,
           [ 0.,  0.,  0., ..., 10.,  0.,  0.],
           [ 0.,  0.,  0., ...,  0.,  9.,  0.],
           [ 0.,  0.,  0., ...,  0.,  0., 11.]])




```python
# Now, also we're going to check that z^TD1 = 0 where 1 is a vector of ones
z = transform(A,y)
ones = np.ones(len(y))
# We need to use the np.isclose function as there are some round off errors when
# computing it.
np.isclose(z@D@ones,0)
```




    True




```python
# Now let's check if our new way of computing the norm cut term yields the same
# result as our old way. Again, we use np.isclose for the same reason
np.isclose(z@(D-A)@z/(z@D@z),normcut(A,y))
```




    True




```python
# We'll do the same with our random cluster vector random_y
z = transform(A,random_y)
np.isclose(z@(D-A)@z/(z@D@z),normcut(A,random_y))
```




    True



# Part D #
Let's begin our optimization. As stated above we want to minimize $N_A(C_0,C_1)$.\
\
With our nifty new expression and tools we can easily define our our problem.\
\
We want the minimum of the function
$$ R_A(z) = \frac{z^\top(D-A)z}{z^\top Dz} $$
subject to the condition $z^\top D1 = 0$. If you remember any of your optimization then we can actually add this condition straight into our optimization, we just have to substitute $z$ for the orthogonal complement of $z$ relative to $D1$. Here's a little function that will handle this.


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
import scipy as sp
# we will use scipy to minimize the function orth_obj which will yield 
# z_ such that R_A(z_) is minimized
# recall that this will still have our condition baked into it.
p = np.ones(len(y))
z_ = scipy.optimize.minimize(orth_obj,p).x
z_
```




    array([ 3.5603426 ,  3.55638481, -1.89197621, -1.5893957 , -1.80206539,
           -1.80407632, -1.89511063, -1.77782978,  3.89028369,  4.0178812 ,
            3.57923807, -1.70387672,  3.28746967,  3.84971425,  3.91496918,
            3.67364763,  3.86831708, -1.76662002, -1.65798997, -1.8016349 ,
            4.08534799,  3.65649925,  4.13642565, -1.5893957 , -2.11110936,
            3.86995243, -1.89994202,  3.59925687,  4.00016407, -1.73118999,
           -1.77542003,  3.52701784,  4.03817119,  3.41492707,  3.87992775,
            3.83007635, -2.11110936,  4.12467759,  3.63839673, -1.92844155,
            3.79048606, -1.74890794, -1.59103426, -2.09277769, -1.61448824,
           -1.73420567, -1.64792837,  3.95357853,  4.15142053,  3.28746965,
           -1.90100973, -1.89868559,  4.138491  ,  3.84453936, -1.85995469,
           -1.88777401,  3.95083953,  3.89846316,  3.749631  , -1.77047896,
           -2.11061465, -2.16545441,  3.27411737, -1.82864424,  3.74015149,
           -1.92722554, -2.11061465, -1.73964379, -1.90015615,  4.25623965,
            3.07870795,  3.97796119,  4.01654974, -2.13233436, -1.61679146,
           -1.85606829,  4.13109266, -1.85752261,  4.37116746, -2.04925283,
           -1.71092962, -1.79460555, -2.10926084,  3.70029575,  3.79048606,
            3.8732161 ,  3.79048606, -2.27164492, -1.64920739, -1.72565098,
           -1.89565201,  3.64057447, -1.69264382, -2.26938198,  3.89846316,
            4.13932127,  4.37116747,  3.97261097, -1.77530148, -2.02577796,
            4.25149306,  3.90264342,  4.08831998, -2.50925972,  4.04057569,
            3.69215449, -2.16850739, -1.58066548,  3.82109824,  3.8527452 ,
           -1.79846904, -1.89144648, -1.64942286,  3.67514895, -2.11151486,
           -1.86237679, -2.52694803, -2.01653082,  3.89846316,  3.91828441,
            3.86872283, -1.80194599,  4.01325874,  4.04375922,  3.89460389,
           -2.11480099,  3.95083953, -2.31569188,  4.02794905,  3.52141165,
           -2.22998713, -2.01817048, -1.76662003, -2.15082456,  3.83242707,
            4.25235587,  3.79450065,  3.72120996,  3.84060737,  3.43432143,
            4.13109265,  3.93157513, -1.49093367, -2.02404945,  3.5603426 ,
           -1.73751404, -1.53468728, -1.77530148, -1.73118999, -1.77574057,
           -1.71393573, -1.86039994, -1.61679146,  3.86995243,  3.91496918,
           -2.23347565,  3.79065266, -1.90199435, -1.49186782, -1.48820253,
            3.81425402,  3.83791119, -1.80407632, -1.9008394 ,  4.14024365,
            3.87919333,  3.70029574, -1.89880856, -2.01653082, -2.26127598,
            4.11704207, -1.77530148,  3.73643462,  3.70355054,  3.62533791,
           -1.88936536, -1.98463287, -1.77530148, -1.58066548,  4.04375922,
            3.24113514, -1.36588807, -1.53468728,  4.13109265,  3.89028369,
            3.80786074, -1.76864076,  3.87992776,  3.83007636,  4.02413014,
           -1.88936536,  3.81425403, -1.89529443,  3.81056068,  3.80214003,
            3.67364763,  3.86995243, -1.5893957 , -1.73420568, -1.46192509])



You'll notice that $z_$ contains more than two different values but before we had said that 
$$ z_i = \begin{cases} \frac{1}{vol(C_0)} \hspace{1cm} \text{    if } y_i = 0\\ -\frac{1}{vol(C_1)}\hspace{1cm} \text{    if } y_i =1 \end{cases}$$
Well we are cheating. That's right, there actually was a little radio in our head and under our hat there was some kind of antenna and some guy miles away from here was giving us all the answers.\
\
This sort of cheating has a name, allegedly. It's called the *continuous relaxation* of the normcut problem.

# Part E #
It's okay that we cheated though. As you can see above, the sign of $z\text{_}[i]$ encodes which cluster the $i$th data point belongs to.\
\
Let's make a little plot to see how our $z\text{_} $ does. Remember if $z$_$[i] \ge 0$ then data point $i$ belongs to $C_0$ and if $z\text{_}[i] < 0$ then data point $i$ belongs to $C_1$.


```python
# This should all be familiar to you.
# We're gonna do what we said above
color_code = (z_<0).astype(int)



plt.scatter(X[:,0], X[:,1],c = color_code)
```




    <matplotlib.collections.PathCollection at 0x27b405625c8>




    
![png](output_39_1.png)
    


Well would you look at that. Looks right to me. Good job computer.

# Part F #
No, we're not done yet.\
\
Unfortunately, our method above is still quite slow. Remember what we're doing? *SPECTRAL* clustering. I mean sure we've used matrices but where does the real beauty of linear algebra come from? That's right all things eigen. Take a moment to praise the sun for eigenvalues and eigenvectors, or the moon or any other celestial body you may worship.\
\
So, we're still going to be minimizing
$$ R_A(z) = \frac{z^\top(D-A)z}{z^\top Dz} $$
subject to the condition $z^\top D1 = 0$\
\
The Rayleigh-Ritz Theorem: minimizing $z$ is equivalent to solving
$$ (D-A)z = \lambda Dz, \hspace{2cm} z^\top D1 =0$$
and this happens to be equivalent to 
$$ D^{-1}(D-A)z = \lambda z, \hspace{2cm} z^\top 1 = 0$$
As math tends to do, magically, $1$ is the eigenvector with the smalles eigenvalue of the matrix $D^{-1}(D-1)$\
\
And $z$ is the eigenvector with the second smallest eigenvalue.\
\
Let the optimization commence (again)\
First, we'll compute $L = D^{-1}(D-A)$ which is called the (normalized) Laplacian matrix of $A$


```python
D_inv = np.linalg.inv(D)
L = D_inv@(D-A)
L
```




    array([[ 1.        ,  0.        ,  0.        , ...,  0.        ,
             0.        ,  0.        ],
           [ 0.        ,  1.        ,  0.        , ...,  0.        ,
             0.        ,  0.        ],
           [ 0.        ,  0.        ,  1.        , ...,  0.        ,
             0.        ,  0.        ],
           ...,
           [ 0.        ,  0.        ,  0.        , ...,  1.        ,
             0.        , -0.1       ],
           [ 0.        ,  0.        ,  0.        , ...,  0.        ,
             1.        ,  0.        ],
           [ 0.        ,  0.        ,  0.        , ..., -0.09090909,
             0.        ,  1.        ]])



Now we compute the eigenvectors and eigenvalues of $L$


```python
vals, vecs = np.linalg.eig(L)
```

We're only interested in the second smallest eigenvalue and its associated eigenvector. So, lets extract those.


```python
# We can find the second smallest eigenvalue as follows
second = np.partition(vals, 1)[1]
# we want the index of the second smallest eigenvalue so we can
# get its eigenvector. 
column = np.where(vals == second)[0][0]
# Note: the eigenvectors are the columns of vecs and NOT the rows)
z_eig = vecs[:,column]
z_eig
```




    array([ 0.09993215,  0.09993215, -0.00368324, -0.00368324, -0.00368324,
           -0.00368324, -0.00368324, -0.00368324,  0.09993215,  0.09993215,
            0.09993215, -0.00368324,  0.09993215,  0.09993215,  0.09993215,
            0.09993215,  0.09993215, -0.00368324, -0.00368324, -0.00368324,
            0.09993215,  0.09993215,  0.09993215, -0.00368324, -0.00368324,
            0.09993215, -0.00368324,  0.09993215,  0.09993215, -0.00368324,
           -0.00368324,  0.09993215,  0.09993215,  0.09993215,  0.09993215,
            0.09993215, -0.00368324,  0.09993215,  0.09993215, -0.00368324,
            0.09993215, -0.00368324, -0.00368324, -0.00368324, -0.00368324,
           -0.00368324, -0.00368324,  0.09993215,  0.09993215,  0.09993215,
           -0.00368324, -0.00368324,  0.09993215,  0.09993215, -0.00368324,
           -0.00368324,  0.09993215,  0.09993215,  0.09993215, -0.00368324,
           -0.00368324, -0.00368324,  0.09993215, -0.00368324,  0.09993215,
           -0.00368324, -0.00368324, -0.00368324, -0.00368324,  0.09993215,
            0.09993215,  0.09993215,  0.09993215, -0.00368324, -0.00368324,
           -0.00368324,  0.09993215, -0.00368324,  0.09993215, -0.00368324,
           -0.00368324, -0.00368324, -0.00368324,  0.09993215,  0.09993215,
            0.09993215,  0.09993215, -0.00368324, -0.00368324, -0.00368324,
           -0.00368324,  0.09993215, -0.00368324, -0.00368324,  0.09993215,
            0.09993215,  0.09993215,  0.09993215, -0.00368324, -0.00368324,
            0.09993215,  0.09993215,  0.09993215, -0.00368324,  0.09993215,
            0.09993215, -0.00368324, -0.00368324,  0.09993215,  0.09993215,
           -0.00368324, -0.00368324, -0.00368324,  0.09993215, -0.00368324,
           -0.00368324, -0.00368324, -0.00368324,  0.09993215,  0.09993215,
            0.09993215, -0.00368324,  0.09993215,  0.09993215,  0.09993215,
           -0.00368324,  0.09993215, -0.00368324,  0.09993215,  0.09993215,
           -0.00368324, -0.00368324, -0.00368324, -0.00368324,  0.09993215,
            0.09993215,  0.09993215,  0.09993215,  0.09993215,  0.09993215,
            0.09993215,  0.09993215, -0.00368324, -0.00368324,  0.09993215,
           -0.00368324, -0.00368324, -0.00368324, -0.00368324, -0.00368324,
           -0.00368324, -0.00368324, -0.00368324,  0.09993215,  0.09993215,
           -0.00368324,  0.09993215, -0.00368324, -0.00368324, -0.00368324,
            0.09993215,  0.09993215, -0.00368324, -0.00368324,  0.09993215,
            0.09993215,  0.09993215, -0.00368324, -0.00368324, -0.00368324,
            0.09993215, -0.00368324,  0.09993215,  0.09993215,  0.09993215,
           -0.00368324, -0.00368324, -0.00368324, -0.00368324,  0.09993215,
            0.09993215, -0.00368324, -0.00368324,  0.09993215,  0.09993215,
            0.09993215, -0.00368324,  0.09993215,  0.09993215,  0.09993215,
           -0.00368324,  0.09993215, -0.00368324,  0.09993215,  0.09993215,
            0.09993215,  0.09993215, -0.00368324, -0.00368324, -0.00368324])



Now we plot the data again with our new vector $z\text{_}eig$ similarly to before.


```python
color_code = (z_eig<0).astype(int)


plt.scatter(X[:,0], X[:,1],c = color_code)
```




    <matplotlib.collections.PathCollection at 0x27b40562388>




    
![png](output_48_1.png)
    



```python
z = (z_eig<0).astype(int)
z
```




    array([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
           0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
           1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
           1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
           1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
           1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
           1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
           0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
           1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
           1, 1])



Yup, looks correct to me.

# Part H #
Great, now let's make a function that can do all we need to classify data points starting with a set of data points $X$ and an numerical argument *epsilon* as before.
# spectral_clustering(X,epsilon) #


```python
def spectral_clustering(X,epsilon = .2):
    """
    This function applies spectral clustering to a set of two dimensional data points
    X using the method that relies on the Rayleigh-Ritz Theorem. 
    
    Inputs:
            X: a set of two dimensional data points (numpy array with shape (n,2))
            epsilon: positive real number, determines the distance threshold when
                    constructing the similarity matrix of X as follows
                    Let A be the similarity matrix of X.
                    If (X[i]-X[j])**2 < epsilon**2 then A[i][j] = 1 
                    otherwise A[i][j] = 0
                    
    Output: 
            z: a binary array of length equal to the number of data points
               that encodes the classification of each data point 
               (ie if z[i] = 0 then X[i] belongs to cluster 0
               if z[i] = 1 then X[i] belongs to cluster 1) 
    """
    
    # Contruct the similarity matrix using our existing function
    A = similarity_matrix(epsilon,X)
    
    # Construct the diagonal matrix D such that
    # D[i][i] = the sum of the ith row of A
    D = np.zeros(np.shape(A))
    np.fill_diagonal(D,A.sum(axis = 1))
    
    # Construct the normalized Laplacian matrix of A
    D_inv = np.linalg.inv(D)
    L = D_inv@(D-A)
    
    # compute the eigenvalues and corresponding eigenvectors of L
    vals, vecs = np.linalg.eig(L)
    
    # extract the eigenvector of the second smallest eigenvalue
    column = np.where(vals == np.partition(vals, 1)[1])[0][0]
    z_eig = vecs[:,column]
    
    # encode the classification of the data points
    # based on the sign of the elements in z_eig and return the binary array
    return (z_eig < 0).astype(int)
    
```

Let's check to make sure our function is working properly. Using the same set of data points and the same value for epsilon as before.


```python
z_test = spectral_clustering(X,0.4)
plt.scatter(X[:,0], X[:,1],c = z_test)
```




    <matplotlib.collections.PathCollection at 0x27b406012c8>




    
![png](output_54_1.png)
    


Looks good. Seems like our function is working.

# Part H #
More testing of the spectral clustering.


```python
# make_mons with noise = 0
X,y = datasets.make_moons(n_samples = n,shuffle=True,noise = 0)
plt.scatter(X[:,0],X[:,1])
```




    <matplotlib.collections.PathCollection at 0x27b331e1508>




    
![png](output_57_1.png)
    



```python
# using the true clusters
plt.scatter(X[:,0],X[:,1],c = y)
```




    <matplotlib.collections.PathCollection at 0x27b3323c888>




    
![png](output_58_1.png)
    



```python
# using spectral clustering with epsilon = 0.4
plt.scatter(X[:,0],X[:,1],c = spectral_clustering(X,.4))
```




    <matplotlib.collections.PathCollection at 0x27b332df948>




    
![png](output_59_1.png)
    


Pretty bad, let's try epsilon = 0.7


```python
plt.scatter(X[:,0],X[:,1],c = spectral_clustering(X,.7))
```




    <matplotlib.collections.PathCollection at 0x27b3335fcc8>




    
![png](output_61_1.png)
    


That's a bingo.


```python
# let's try to increase the noise slightly
X,y = datasets.make_moons(n_samples = n,shuffle=True,noise = .09)
plt.scatter(X[:,0],X[:,1])
```




    <matplotlib.collections.PathCollection at 0x27b333e89c8>




    
![png](output_63_1.png)
    


We can still mostly make out the two half moons by looking at them.


```python
# We'll look at the true clusters so we can compare
plt.scatter(X[:,0],X[:,1],c=y)
```




    <matplotlib.collections.PathCollection at 0x27b33456b48>




    
![png](output_65_1.png)
    



```python
# Okay, epsilon = 4 gives an error as it builds a singular matrix so let's do 
# epsilon = .5
plt.scatter(X[:,0],X[:,1],c = spectral_clustering(X,.5))


```




    <matplotlib.collections.PathCollection at 0x27b334de488>




    
![png](output_66_1.png)
    


Well that worked immediately.
Let's turn up the noise some more.


```python
X,y = datasets.make_moons(n_samples = n,shuffle=True,noise = .2)
plt.scatter(X[:,0],X[:,1])
```




    <matplotlib.collections.PathCollection at 0x27b353dab48>




    
![png](output_68_1.png)
    


I can vaguely make it out but this looks like a challenge. Let's see the true clusters


```python
plt.scatter(X[:,0],X[:,1],c=y)
```




    <matplotlib.collections.PathCollection at 0x27b354733c8>




    
![png](output_70_1.png)
    


I'll be surprised if spectral clustering can match that.


```python
# Lets try epsilon = .5
plt.scatter(X[:,0],X[:,1],c = spectral_clustering(X,.6))
```




    <matplotlib.collections.PathCollection at 0x27b3553ed88>




    
![png](output_72_1.png)
    


Pretty bad but It's as good as we'll get I think.


```python
# Last one I promise. Let's set noise to .4
X,y = datasets.make_moons(n_samples = n,shuffle=True,noise = .4)
plt.scatter(X[:,0],X[:,1])
```




    <matplotlib.collections.PathCollection at 0x27b3f2adb08>




    
![png](output_74_1.png)
    


Well, I can't make this out at all.


```python
plt.scatter(X[:,0],X[:,1],c=y)
```




    <matplotlib.collections.PathCollection at 0x27b3f32be08>




    
![png](output_76_1.png)
    


Right. Okay, let's see if spectral clustering can find clusters that make more sense.


```python
# epsilon = 0.79
plt.scatter(X[:,0],X[:,1],c = spectral_clustering(X,.79))
```




    <matplotlib.collections.PathCollection at 0x27b3f39fa48>




    
![png](output_78_1.png)
    


I'd be more inclined to say that these are actual clusters. They don't look like moons though.

# Part I #
Bull's eye.\
\
More spectral clustering tests


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x27b3f42a148>




    
![png](output_81_1.png)
    



```python
# Let's see the actual clusters just, you know, to be sure
plt.scatter(X[:,0], X[:,1],c=y)
```




    <matplotlib.collections.PathCollection at 0x27b405a7a48>




    
![png](output_82_1.png)
    



```python
# What can k-means do with this
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x27b4075bac8>




    
![png](output_83_1.png)
    


Hmm not a very good job in seperating the circles. Time for spectral clustering... again


```python
# epsilon = 0.4
plt.scatter(X[:,0],X[:,1],c = spectral_clustering(X,0.4))
```




    <matplotlib.collections.PathCollection at 0x27b40888ac8>




    
![png](output_85_1.png)
    


We got him. Good job. This concludes our scheduled programming for this evening. Thank you and good night.
