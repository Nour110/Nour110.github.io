---
layout: post
title: Blog Post 6
---

Welcome to another blog post. Today we are going to be using tensorflow again. We will be using it to classify text this time. Specificly, we will be training a model that can classify an article as "fake news" or not. This blog post was also written using Google Colab.

We begin with importing the packages we will need.


```python
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import re
import string

import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from nltk.corpus import stopwords

import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
```

Now we will import the data set that we will use to train our model.


```python
data_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
data_frame = pd.read_csv(data_url)
```

Let's take a quick look at the data.\
Note that for the "fake" column, a value of `0` means that the article is true and `1` means that the article contains fake news.


```python
data_frame = data_frame[["title","text", "fake"]]
data_frame
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
      <td>If Clinton and Lynch just talked about grandki...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>Can Pence's vow not to sling mud survive a Tru...</td>
      <td>() - In 1990, during a close and bitter congre...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
      <td>A new ad by the Hillary Clinton SuperPac Prior...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>Trump celebrates first 100 days as president, ...</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
      <td>MELBOURNE, FL is a town with a population of 7...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 3 columns</p>
</div>



So, we're gonna prep our data for use in our model.

There are some words that won't really help our model decipher much so we are going to exclude them. Some examples of these words are "the", "and", or "but".\
Usually we'd call them stopwords and we are going to use the `nltk.corpus` `stopwords` package to help us remove these words from the title and text of our articles.


```python
# We'll make a list of the stopwords which we will call stop
stop = stopwords.words('english')
```

Let's make a function to clean the data of the stopwords and construct a `tf.data.Dataset`. Our `tf.data.Dataset` will have two inputs and one output. The inputs will be the title and text from our data and the output will be the fake column from our dataframe.\
We will also batch our data into batches of 100.


```python
def make_dataset(df):
  df["title"] = df["title"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df["text"] = df["text"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  data = tf.data.Dataset.from_tensor_slices(
      (
        {
              "title": df[["title"]],
              "text": df[["text"]]
        },
       {
           "fake":df[["fake"]]
       }
      )
  )
  data = data.batch(100)
  return data
  
```


```python
data = make_dataset(data_frame)
```


```python
data
```




    <BatchDataset shapes: ({title: (None, 1), text: (None, 1)}, {fake: (None, 1)}), types: ({title: tf.string, text: tf.string}, {fake: tf.int64})>



Next we will split our data into training data and validation data.


```python
train_size = int(0.8*len(data))
val_size = int(0.2*len(data))
train = data.take(train_size)
val = data.skip(train_size).take(val_size)
```

Our base rate is the accuracy of a model that always guesses whichever value comes up most. So, let's see whether most of the articles contain fake news or not.


```python
labels_iterator = train.unbatch().map(lambda title,fake: fake).as_numpy_iterator()
fake_count = 0
real_count = 0
for i in labels_iterator:
  if i["fake"] == 0:
    real_count += 1
  else:
    fake_count += 1
```


```python
real_count,fake_count
```




    (8603, 9397)



As you can see above, there are more articles labeled as fake than true. Thus, our base model will always guess fake and it will be correct about 52.26% of the time.\
Let's see if we can't train a model that is more accurate than that!\
We're going to experiment and create three models in total and see are we better only evaluating based on the text of the article or the title of the article or both.

# First Model #
Our first model will train only using the article title!\
Let's see how well it does.\
Before we start, we have to vectorize our text. Spooky.\
To learn more about what that means, refer to the following link:\
https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization


```python
vectorize = TextVectorization(
    max_tokens=2000, standardize='lower_and_strip_punctuation',
    split='whitespace', ngrams=2, output_mode='int'
)
```


```python
vectorize.adapt(train.map(lambda x, y: x["title"]))
```

Now, we will specify the input of our model


```python
titles_input = keras.Input(
    shape = (1,),
    name = "title",
    dtype = "string"
)
```

We can pick the hidden layers of our model!


Note we are not gonna be using the Sequential API of TensorFlow this time but instead we will be using the Functional API. As always, to learn more:\
https://www.tensorflow.org/guide/keras/functional


```python
titles_features = vectorize(titles_input)
titles_features = layers.Embedding(2000, 3, name = "embedding")(titles_features)
titles_features = layers.Dropout(0.2)(titles_features)
titles_features = layers.GlobalAveragePooling1D()(titles_features)
titles_features = layers.Dropout(0.2)(titles_features)
titles_features = layers.Dense(32, activation="relu")(titles_features)
```

And finally we specify the output. Recall we have only 2 options either fake or not fake.


```python
output = layers.Dense(2,name = "fake")(titles_features)
```

Now let's run our fancy new model.


```python
model = keras.Model(inputs = titles_input, outputs = output)
```


```python
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model.fit(train, 
                    validation_data=val,
                    epochs = 25)
```

    Epoch 1/25
    

    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning:
    
    Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
    
    

    180/180 [==============================] - 1s 5ms/step - loss: 0.6171 - accuracy: 0.7767 - val_loss: 0.4474 - val_accuracy: 0.8914
    Epoch 2/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.3359 - accuracy: 0.8883 - val_loss: 0.2387 - val_accuracy: 0.9216
    Epoch 3/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.2352 - accuracy: 0.9154 - val_loss: 0.1837 - val_accuracy: 0.9355
    Epoch 4/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.1927 - accuracy: 0.9312 - val_loss: 0.1520 - val_accuracy: 0.9456
    Epoch 5/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.1626 - accuracy: 0.9443 - val_loss: 0.1291 - val_accuracy: 0.9557
    Epoch 6/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.1384 - accuracy: 0.9517 - val_loss: 0.1105 - val_accuracy: 0.9625
    Epoch 7/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.1207 - accuracy: 0.9582 - val_loss: 0.0960 - val_accuracy: 0.9674
    Epoch 8/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.1058 - accuracy: 0.9623 - val_loss: 0.0843 - val_accuracy: 0.9708
    Epoch 9/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0946 - accuracy: 0.9663 - val_loss: 0.0749 - val_accuracy: 0.9733
    Epoch 10/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0867 - accuracy: 0.9707 - val_loss: 0.0678 - val_accuracy: 0.9768
    Epoch 11/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0780 - accuracy: 0.9739 - val_loss: 0.0620 - val_accuracy: 0.9793
    Epoch 12/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0728 - accuracy: 0.9745 - val_loss: 0.0578 - val_accuracy: 0.9798
    Epoch 13/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0668 - accuracy: 0.9773 - val_loss: 0.0544 - val_accuracy: 0.9809
    Epoch 14/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0611 - accuracy: 0.9789 - val_loss: 0.0521 - val_accuracy: 0.9816
    Epoch 15/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0596 - accuracy: 0.9791 - val_loss: 0.0503 - val_accuracy: 0.9827
    Epoch 16/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0571 - accuracy: 0.9797 - val_loss: 0.0490 - val_accuracy: 0.9827
    Epoch 17/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0553 - accuracy: 0.9801 - val_loss: 0.0477 - val_accuracy: 0.9829
    Epoch 18/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0522 - accuracy: 0.9812 - val_loss: 0.0469 - val_accuracy: 0.9838
    Epoch 19/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0516 - accuracy: 0.9812 - val_loss: 0.0464 - val_accuracy: 0.9836
    Epoch 20/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0507 - accuracy: 0.9803 - val_loss: 0.0466 - val_accuracy: 0.9825
    Epoch 21/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0472 - accuracy: 0.9832 - val_loss: 0.0457 - val_accuracy: 0.9831
    Epoch 22/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0448 - accuracy: 0.9841 - val_loss: 0.0457 - val_accuracy: 0.9827
    Epoch 23/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0436 - accuracy: 0.9839 - val_loss: 0.0454 - val_accuracy: 0.9834
    Epoch 24/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0432 - accuracy: 0.9836 - val_loss: 0.0459 - val_accuracy: 0.9838
    Epoch 25/25
    180/180 [==============================] - 1s 4ms/step - loss: 0.0432 - accuracy: 0.9844 - val_loss: 0.0462 - val_accuracy: 0.9825
    


```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f3ab24c6f10>




    
![BP6_0]({{ site.baseurl }}/images/BP6_0.png)
    


Those results were not at all expected. We reached 98% validation accuracy using only the title of the articles.\
This was my first attempt and I really didn't try much else. These results are suspiciously good.\
Let's see if our next model can do better.

# Model 2 #
Our second model will only take the article's text as input and ignore the title of the article.\
We will use the same layers as the previous model so we can have a fair assessment.\
The steps will be pretty much exactly the same!


```python
vectorize.adapt(train.map(lambda x, y: x["text"]))
vocab = np.array(vectorize.get_vocabulary())
text_input = keras.Input(
    shape = (1,),
    name = "text",
    dtype = "string"
)
```


```python
text_features = vectorize(text_input)
text_features = layers.Embedding(2000, 3, name = "embedding")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation="relu")(text_features)

output = layers.Dense(2,name = "fake")(text_features)
```


```python
model = keras.Model(inputs = text_input, outputs = output)
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

```


```python
history = model.fit(train, 
                    validation_data=val,
                    epochs = 25)
```

    Epoch 1/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.6174 - accuracy: 0.7591 - val_loss: 0.5305 - val_accuracy: 0.8811
    Epoch 2/25
    180/180 [==============================] - 6s 32ms/step - loss: 0.4517 - accuracy: 0.8961 - val_loss: 0.3625 - val_accuracy: 0.9344
    Epoch 3/25
    180/180 [==============================] - 6s 33ms/step - loss: 0.3314 - accuracy: 0.9246 - val_loss: 0.2739 - val_accuracy: 0.9483
    Epoch 4/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.2640 - accuracy: 0.9381 - val_loss: 0.2253 - val_accuracy: 0.9544
    Epoch 5/25
    180/180 [==============================] - 6s 32ms/step - loss: 0.2256 - accuracy: 0.9461 - val_loss: 0.1946 - val_accuracy: 0.9591
    Epoch 6/25
    180/180 [==============================] - 6s 33ms/step - loss: 0.1990 - accuracy: 0.9520 - val_loss: 0.1738 - val_accuracy: 0.9622
    Epoch 7/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.1816 - accuracy: 0.9557 - val_loss: 0.1587 - val_accuracy: 0.9629
    Epoch 8/25
    180/180 [==============================] - 6s 33ms/step - loss: 0.1662 - accuracy: 0.9593 - val_loss: 0.1473 - val_accuracy: 0.9674
    Epoch 9/25
    180/180 [==============================] - 6s 33ms/step - loss: 0.1534 - accuracy: 0.9621 - val_loss: 0.1380 - val_accuracy: 0.9690
    Epoch 10/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.1434 - accuracy: 0.9643 - val_loss: 0.1300 - val_accuracy: 0.9690
    Epoch 11/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.1386 - accuracy: 0.9636 - val_loss: 0.1245 - val_accuracy: 0.9699
    Epoch 12/25
    180/180 [==============================] - 6s 33ms/step - loss: 0.1308 - accuracy: 0.9668 - val_loss: 0.1194 - val_accuracy: 0.9715
    Epoch 13/25
    180/180 [==============================] - 6s 32ms/step - loss: 0.1224 - accuracy: 0.9691 - val_loss: 0.1122 - val_accuracy: 0.9717
    Epoch 14/25
    180/180 [==============================] - 6s 35ms/step - loss: 0.1187 - accuracy: 0.9685 - val_loss: 0.1089 - val_accuracy: 0.9735
    Epoch 15/25
    180/180 [==============================] - 6s 36ms/step - loss: 0.1132 - accuracy: 0.9705 - val_loss: 0.1046 - val_accuracy: 0.9746
    Epoch 16/25
    180/180 [==============================] - 6s 33ms/step - loss: 0.1087 - accuracy: 0.9702 - val_loss: 0.1007 - val_accuracy: 0.9755
    Epoch 17/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.1050 - accuracy: 0.9727 - val_loss: 0.0983 - val_accuracy: 0.9753
    Epoch 18/25
    180/180 [==============================] - 6s 35ms/step - loss: 0.1033 - accuracy: 0.9732 - val_loss: 0.0960 - val_accuracy: 0.9764
    Epoch 19/25
    180/180 [==============================] - 7s 36ms/step - loss: 0.0971 - accuracy: 0.9729 - val_loss: 0.0929 - val_accuracy: 0.9755
    Epoch 20/25
    180/180 [==============================] - 7s 37ms/step - loss: 0.0955 - accuracy: 0.9714 - val_loss: 0.0902 - val_accuracy: 0.9771
    Epoch 21/25
    180/180 [==============================] - 6s 36ms/step - loss: 0.0912 - accuracy: 0.9749 - val_loss: 0.0911 - val_accuracy: 0.9773
    Epoch 22/25
    180/180 [==============================] - 7s 36ms/step - loss: 0.0848 - accuracy: 0.9765 - val_loss: 0.0860 - val_accuracy: 0.9786
    Epoch 23/25
    180/180 [==============================] - 6s 36ms/step - loss: 0.0855 - accuracy: 0.9743 - val_loss: 0.0847 - val_accuracy: 0.9789
    Epoch 24/25
    180/180 [==============================] - 6s 35ms/step - loss: 0.0817 - accuracy: 0.9757 - val_loss: 0.0835 - val_accuracy: 0.9786
    Epoch 25/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.0813 - accuracy: 0.9759 - val_loss: 0.0825 - val_accuracy: 0.9764
    


```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f3ab17bd5d0>




    
![BP6_1]({{ site.baseurl }}/images/BP6_1.png)
    


Well, about the same accuracy at 97.64% but clearly lower even if only slightly. Let's see if using both the text and the title can do any better.

# Model 3 #

We're going to create a model that will consider both the title and the text of the article.\
We already have a pipeline for each one seperately so what we'll do is concatenate the output of the `text` only pipeline with the `title` only pipeline.\
BEHOLD BIRDMAN!\
Actually, we have to modify one of our pipelines slightly.


```python
text_features = vectorize(text_input)
text_features = layers.Embedding(2000, 3, name = "birdman")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation="relu")(text_features)

output = layers.Dense(2,name = "fake")(text_features)
```

Okay, now we can concatenate.\
BEHOLD BIRDMAN! 


```python
main = layers.concatenate([text_features,titles_features],axis = 1)
```


```python
main = layers.Dense(32,activation = "relu")(main)
output = layers.Dense(2,name="fake")(main)
```

Now we create and run our model.


```python
model = keras.Model(inputs = [text_input,titles_input], outputs = output)
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

```


```python
history = model.fit(train, 
                    validation_data=val,
                    epochs = 25)
```

    Epoch 1/25
    180/180 [==============================] - 7s 36ms/step - loss: 0.5199 - accuracy: 0.7618 - val_loss: 0.2402 - val_accuracy: 0.9157
    Epoch 2/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.2299 - accuracy: 0.9153 - val_loss: 0.1780 - val_accuracy: 0.9355
    Epoch 3/25
    180/180 [==============================] - 6s 35ms/step - loss: 0.1838 - accuracy: 0.9326 - val_loss: 0.1445 - val_accuracy: 0.9463
    Epoch 4/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.1527 - accuracy: 0.9451 - val_loss: 0.1194 - val_accuracy: 0.9546
    Epoch 5/25
    180/180 [==============================] - 7s 37ms/step - loss: 0.1224 - accuracy: 0.9559 - val_loss: 0.0910 - val_accuracy: 0.9690
    Epoch 6/25
    180/180 [==============================] - 6s 36ms/step - loss: 0.0962 - accuracy: 0.9680 - val_loss: 0.0696 - val_accuracy: 0.9766
    Epoch 7/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.0767 - accuracy: 0.9749 - val_loss: 0.0534 - val_accuracy: 0.9822
    Epoch 8/25
    180/180 [==============================] - 6s 35ms/step - loss: 0.0585 - accuracy: 0.9805 - val_loss: 0.0451 - val_accuracy: 0.9867
    Epoch 9/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.0459 - accuracy: 0.9859 - val_loss: 0.0364 - val_accuracy: 0.9901
    Epoch 10/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.0393 - accuracy: 0.9881 - val_loss: 0.0301 - val_accuracy: 0.9910
    Epoch 11/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.0325 - accuracy: 0.9897 - val_loss: 0.0277 - val_accuracy: 0.9915
    Epoch 12/25
    180/180 [==============================] - 6s 35ms/step - loss: 0.0286 - accuracy: 0.9913 - val_loss: 0.0269 - val_accuracy: 0.9915
    Epoch 13/25
    180/180 [==============================] - 6s 33ms/step - loss: 0.0249 - accuracy: 0.9928 - val_loss: 0.0247 - val_accuracy: 0.9915
    Epoch 14/25
    180/180 [==============================] - 6s 35ms/step - loss: 0.0226 - accuracy: 0.9932 - val_loss: 0.0237 - val_accuracy: 0.9915
    Epoch 15/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.0219 - accuracy: 0.9931 - val_loss: 0.0235 - val_accuracy: 0.9915
    Epoch 16/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.0194 - accuracy: 0.9945 - val_loss: 0.0221 - val_accuracy: 0.9924
    Epoch 17/25
    180/180 [==============================] - 6s 33ms/step - loss: 0.0185 - accuracy: 0.9944 - val_loss: 0.0233 - val_accuracy: 0.9910
    Epoch 18/25
    180/180 [==============================] - 6s 33ms/step - loss: 0.0168 - accuracy: 0.9953 - val_loss: 0.0202 - val_accuracy: 0.9933
    Epoch 19/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.0207 - val_accuracy: 0.9926
    Epoch 20/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.0136 - accuracy: 0.9952 - val_loss: 0.0237 - val_accuracy: 0.9915
    Epoch 21/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.0143 - accuracy: 0.9956 - val_loss: 0.0199 - val_accuracy: 0.9933
    Epoch 22/25
    180/180 [==============================] - 6s 34ms/step - loss: 0.0112 - accuracy: 0.9964 - val_loss: 0.0203 - val_accuracy: 0.9919
    Epoch 23/25
    180/180 [==============================] - 7s 37ms/step - loss: 0.0124 - accuracy: 0.9964 - val_loss: 0.0229 - val_accuracy: 0.9919
    Epoch 24/25
    180/180 [==============================] - 6s 36ms/step - loss: 0.0115 - accuracy: 0.9961 - val_loss: 0.0192 - val_accuracy: 0.9937
    Epoch 25/25
    180/180 [==============================] - 6s 35ms/step - loss: 0.0117 - accuracy: 0.9962 - val_loss: 0.0198 - val_accuracy: 0.9926
    


```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f3ab28fc410>




    
![BP6_2]({{ site.baseurl }}/images/BP6_2.png)
    


Well, can't get much better than that. Clearly this is our best model.\
Based on this, if you were trying to train a model that can detect fake news, I would use both the title and the text of the article.\
Let's see how this model does on the unseen test data.

# Model Evaluation #


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
frame = pd.read_csv(test_url)
frame = frame[["title","text", "fake"]]
test = make_dataset(frame)
```


```python
model.evaluate(test,verbose = 2)
```

    225/225 - 3s - loss: 0.0277 - accuracy: 0.9918 - 3s/epoch - 14ms/step
    




    [0.027726100757718086, 0.9918481707572937]



Amazing results. 99% accuracy on the test data as well.\
Let's take a look at how various words were influenced the classification within our models.

# Embedding Visualization #

The next code block will help us visualize how each word was used to help our model make a classification of an article. Don't worry too much about the details.




```python
weights = model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer


from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
```


```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 3,
                 hover_name = "word")

fig.show()

```
![yikes]({{ site.baseurl }}/images/yikes.png)

Unfortunately, I couldn't download the interactive plot from Google Colab but here is a static image of it.\
Okay, that's a lot of info.\
The higher the `x1` coordinate of a point is the more the word is associated with fake news. Similarly, if the `x0` coordinate is high then that word is associated with an article not containing fake news.\
It seems that the word `borders` was pretty strongly associated to fake news articles and was the model considered it neutral when it comes to classifying something as not fake news.\
Interestingly, the word `just` is associated with both and so is the word `liberal`.\
The word `killed` was has a negative `x0` coordinate and a `x1` coordinate near `0.1` which seems to make sense.\
For whatever reason, the word `of` is really makes the model think the article is not fake news.

Thank you all and have a good night! We will meet again somewhere over the rainbow.
